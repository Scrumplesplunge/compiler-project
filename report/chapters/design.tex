The implementation for this project will be broken into two main parts: the
Occam compiler, and the Transputer virtual machine. The reason for making this
divide is so that it will be possible to develop multiple versions of the
virtual machine, potentially with different objectives such as performance or
debugging. The compiler is implemented in \texttt{Haskell}, since it is well
suited for the task, whilst the virtual machine is implemented in \texttt{C++}
for the sake of performance.

\section{Compiler}

The Occam syntax is designed such that the indentation very much determines the
structure of the program. Whilst this is convenient for humans, it presents an
inconvenience for the compiler: this syntax is \textit{not} context-free. The
meaning of indentation on one line depends upon the indentation on the
surrounding lines. If it has increased, then a new block as started; if it has
decreased, one has ended.

Luckily, it is possible to transform the source code at an early stage so that
it is then context-free. This makes it much more simple to parse. The
transformation is described in the first three sections below.

\subsection{Lexer}

The Lexer is responsible for performing the first stage of the parsing. The
plain text of the source code is taken as input, and a stream of tokens is
produced. Each token consists of one lexical unit from the source code. At this
stage each unit is a terminal token, meaning that there is no sub-structure in
a given token, but a clearly defined meaning.

The Occam syntax has only a handful of different token types:
\begin{itemize}
  \item \texttt{CHAR c} - A character literal, e.g. \texttt{'a'}.
  \item \texttt{COMMENT} - A comment, e.g. \texttt{-- foo}.
  \item \texttt{IDENT x} - An identifier, e.g. \texttt{bar}.
  \item \texttt{INTEGER i} - An integer literal, e.g. \texttt{123}.
  \item \texttt{KEYWORD k} - A program keyword, e.g. \texttt{SEQ}.
  \item \texttt{STRING s} - A string literal, e.g. \texttt{"Hello, World!"}.
  \item \texttt{SYMBOL s} - A symbol, e.g. \texttt{*} or \texttt{:=}.
\end{itemize}
In addition to these tokens, there are two more which are used to convey
indentation and formatting:
\begin{itemize}
  \item \texttt{NEWLINE} - A line break.
  \item \texttt{SPACES n} - A sequence of \texttt{n} spaces.
\end{itemize}
These tokens are used in the Indentation Parsing stage to transform the language
into one which is context-free.

\subsection{Indentation Parsing}

The indentation parser takes the token stream generated by the Lexer, and
produces a different token stream which can be parsed in a context-free manner.
Most tokens are passed through this stage unmodified; the only tokens which are
changed or removed are the \texttt{COMMENT}, \texttt{NEWLINE} and
\texttt{SPACES} tokens.

Indentation parsing is achieved as follows: the token stream is split into
groups of tokens, breaking at each \texttt{NEWLINE} token; the \texttt{COMMENT}
tokens are removed, and \texttt{SPACES} tokens in each line are removed, unless
they are indentation\footnote{i.e. they are the first token on a line, and the
next token is not whitespace.}; finally, the indentation tokens are converted
into \texttt{INDENT} and \texttt{DEDENT} tokens whenever the level of
indentation is increased or decreased between lines.

At this stage, all indentation has been replaced with \texttt{INDENT} and
\texttt{DEDENT} tokens, and the line groups are concatenated back together to
form the resultant token stream. By performing this step, the rest of the syntax
parsing can be performed in a context-free manner.

\subsection{Syntax Parsing}

Taking the output of the indentation parser as input, it is now possible to
parse the grammar with a collection of \gls{cfg} rules. These were sourced in
part from \cite[p.~124]{jones}, and otherwise from
\cite[p.~106]{occ21}\footnote{Although this manual is for a different version of
Occam, there is a reasonable overlap in functionality.}

One possibility for parsing would be to use an existing utility such as
\textit{Happy}\footnote{\url{https://www.haskell.org/happy/}} to generate a
parser from the grammar. However, such utilities often make it harder to produce
useful or accurate syntax error messages, so I have elected to implement the
parser from scratch.

The parser consists of two main parts. The first part is a collection of
functions which describe parsing behaviour for each of a collection of
context-free combinators, as well as reduction rules which are used to construct
the \textit{\gls{ast}} from the source. The second part is an expression of each
of the grammar rules of Occam in terms of these combinators. Together, they form
a complete parser for the Occam language.

\subsection{Semantic Checking}

Successful completion of the parsing phase implies that the source code is
grammatically sound. However, it is still possible that the source code does not
make sense. Consider the following example:
\begin{lstlisting}[language=occam]
CHAN c :
SEQ
  c := 2
\end{lstlisting}
Grammatically, there is nothing wrong with the line \texttt{c := 2}. However,
semantically, it makes no sense to assign an integer value to a channel. Thus,
it is necessary to perform semantic analysis to ensure that the program makes
sense. The semantic analysis stage performs checks that
\begin{itemize}
  \item variables are defined before they are used,
  \item types are not misused in expressions and statements,
  \item constant-expressions are actually compile-time computable,
\end{itemize}
among other checks. The analyser can also warn about things which are not
clearly errors, but might either be a programmer mistake, or could have
unexpected behaviour. Alongside these checks, the analyser is responsible for
performing other transforms to the \gls{ast} in order to make it easier to
generate code from the resulting tree.

\subsection{Code Generation}

Conceptually the simplest step, but practically one of the most challenging,
is code generation. Taking the \gls{ast} as input, each program construct can be
compiled into a sequence of instructions that implement the desired behaviour.
Since program constructs can nest, code generation is a highly recursive
process, and is one of the main reasons behind choosing a functional language to
perform the compilation.

The code generation is factorised into a collection of functions, with at least
one per program construct. Although code generation is mostly a
functionally-pure process, it is necessary to be able to generate unique labels
throughout, and so the functions are implemented monadically.

\subsection{Possibly: Peephole Optimization}

As an optional extra of the project, a peephole optimizer could be added to the
tail-end of the compiler in order to clean up some of the most obvious
bottlenecks. Since this is not essential for getting the compiler to operate
however, it is a low priority component.

\section{Assembler}

Transputer Assembler is a peculiar combination consisting mostly of
\gls{risc}-style instructions, but including a small number of oddly high-level
operations. In reality, the instruction set of the transputer was implemented in
microcode for a much simpler underlying processor, but a virtual machine can
implement these instructions directly.

\subsection{Assembler Syntax}

In order to make parsing simple, the syntax for the assembler is very basic.
Each line in the input consists (in the following order) of:
\begin{itemize}
  \item An optional label, e.g. \texttt{START:}
  \item An optional \gls{mnemonic}, e.g. \texttt{opr}
  \item If an instruction was given, an optional argument. This can either be
        numeric (e.g. \texttt{-123}, \texttt{0xDEADBEEF}) or symbolic
        (e.g. \texttt{FOO} or \texttt{BAR - BAZ}).
  \item An optional comment, e.g. \texttt{\# Hello, World!}
\end{itemize}

This syntax can be parsed by the following regular expression\footnote{For those
unfamiliar with C++11 raw string literals, \texttt{R"foo(bar)foo"} is a string
containing \texttt{bar}. No escape sequences are processed between the markers
\texttt{foo(} and \texttt{)foo}, making them perfect for expressing regular
expressions without having to double-type every backslash, or escape every
double-quote.}:
\lstinputlisting[firstline=129,lastline=143,language=c++]{../vm/src/operations.cc}

\subsection{Symbolic Labels}

The syntax description in the previous section hints at symbolic labels. The
purpose of these labels is to make it easier to mark locations in the source
code for the purpose of jumping to instructions. The most obvious way to assign
values to these labels would be to assign them values based upon where the
instructions are actually located in memory.

However, many of the instructions concerning code locations require the
\textit{relative} location of the instructions, rather than the
\textit{absolute} location. For example, the jump instruction \texttt{J}
requires a relative offset, and computing this relative value manually would be
inconvenient. Therefore, the assembler will have to somehow address this issue
in order to allow instructions like \texttt{J} to take only a label as an
argument, but for other instructions such as \texttt{STARTP} to be able to
compute offsets between labels.

The solution to this is delightfully straightforward: rather than having a label
expand to the absolute location of its definition, the label can be expanded to
the \textit{offset} from the reference
location\textsuperscript{\cite{supervisor}}. This allows \texttt{J} to use the
label as the argument, whilst leaving label difference calculations unaffected.
Absolute locations can be computed with the combination of \texttt{LDC label}
and \texttt{LDPI}\footnote{\texttt{LDPI} loads the value of the instruction
pointer and adds it to the value in register A.}.

\subsection{Variable-length Instructions}

As explained in \ref{ops}, the Transputer has only 13 \gls{direct}s. The rest of
the instruction set is accessed via the \texttt{OPR} instruction, and are
referred to as \gls{indirect}s. However, the assembler can perform this
indirection automatically, allowing the programmer to write \texttt{XOR} and
have the assembler generate the sequence \texttt{PFIX 3, OPR 3} which executes
the same thing. Additionally, direct instructions may take longer arguments by
augmenting them using the \texttt{PFIX} and \texttt{NFIX} instructions.

Having this automatic conversion is convenient for the programmer, but also
introduces a few difficulties in the assembler. Symbolic labels have to be
expanded before the instructions referencing them can be variable-length
encoded. Awkwardly, the value of labels might be affected by the length of
the instructions that reference them. The length of the encoded instruction
might even affect what value the instruction should be encoding.
Consider the following example:

\begin{lstlisting}
FOO:
  ...
  <500 instructions later>
  ...
  j FOO
\end{lstlisting}

The length of the instruction \texttt{j FOO} depends upon how far back
\texttt{FOO} is. However, the label \texttt{FOO} has to be expanded relative to
the \textit{last} byte of the jump, so it is necessary to encode the
instruction, in order to compute the offset for \texttt{FOO}, which is needed so
that the instruction can be encoded in the first place.

To address this, the assembler will need to repeatedly recompute the label
values and the instruction lengths until a fixed point is reached. Luckily, this
is guaranteed to happen. The value of the operand only increases or decreases
linearly with the change in the value of the label, and the length of the
instruction only increases with the log of the offset. This means that given
reasonable initial values for the labels, a bounded number of iterations is
sufficient to find a fixed point. \textcolor{red}{TODO: Maybe calculate this
bound.}

\section{Virtual Machine}

The virtual machine will execute the bytecode produced by the assembler. It will
implement most of the Transputer instruction set, with some deviations.

\subsection{Target Executable Format}

There are two parts to the programs that the compiler will produce:
instructions and data. The instructions will be output as Transputer assembler
which is then assembled into bytecode, but the data has to be provided in a
different manner. My intention is for the compiler to additionally output a
binary blob containing some representation of the initial state of the
Transputer memory. This can then be passed to the virtual machine and be used to
initialize the available RAM before the program is started, or alternatively
linked with the bytecode in an executable wrapper.

\subsection{Basic Functionality - Single Instance}

A single instance of the virtual machine will aim to emulate a single
Transputer. This means that a program running on a single instance will not be
taking advantage of any parallelism.

\subsection{Advanced Functionality - Multiple Instances}

Since Occam processes communicate via channels rather than by sharing memory, it
is relatively trivial to imagine these channels being implemented with atomics
and mutexes in order to allow true parallelism. Alternatively, network sockets
and a simple protocol could allow them to be used between separate physical
machines. I will aim to implement some of this functionality, to allow true
parallelism to be taken advantage of.
