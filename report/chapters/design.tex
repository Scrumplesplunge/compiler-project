The implementation for this project will be broken into three main parts: the
Occam compiler, the \gls{transputer} virtual machine, and the runtime systems.
These components are all conceptually separated, so it is convenient to separate
them in the implementation to make them more understandable. The compiler for
the project is written in \texttt{Haskell}, whilst the virtual machine and the
runtime systems are both written in \texttt{C++}.

\section{Compiler}

The Occam syntax is designed such that the indentation very much determines the
structure of the program. Whilst this is convenient for humans, it presents an
inconvenience for the compiler: this syntax is \textit{not} context-free. The
meaning of indentation on one line depends upon the indentation on the
surrounding lines. If it has increased, then a new block as started; if it has
decreased, one has ended.

Luckily, it is possible to transform the source code at an early stage so that
it is then context-free. This makes it much more simple to parse. The
transformation is described in the first three sections below.

\subsection{Lexer} \label{design-lexer}

The Lexer is responsible for performing the first stage of the parsing. The
plain text of the source code is taken as input, and a stream of tokens is
produced. Each token consists of one lexical unit from the source code. At this
stage each unit is a terminal token, meaning that there is no sub-structure in
a given token, but a clearly defined meaning.

The Occam syntax has only a handful of different token types:
\begin{itemize}
  \item \texttt{CHAR c} - A character literal, e.g. \texttt{'a'}.
  \item \texttt{COMMENT} - A comment, e.g. \texttt{-- foo}.
  \item \texttt{IDENT x} - An identifier, e.g. \texttt{bar}.
  \item \texttt{INTEGER i} - An integer literal, e.g. \texttt{123}.
  \item \texttt{KEYWORD k} - A program keyword, e.g. \texttt{SEQ}.
  \item \texttt{STRING s} - A string literal, e.g. \texttt{"Hello, World!"}.
  \item \texttt{SYMBOL s} - An operator symbol, e.g. \texttt{*} or \texttt{:=}.
\end{itemize}
In addition to these tokens, there are two more which are only important for
parsing the indentation of the program:
\begin{itemize}
  \item \texttt{NEWLINE} - A line break.
  \item \texttt{SPACES n} - A sequence of \texttt{n} spaces.
\end{itemize}
These tokens are removed or substituted in the Indentation Parsing stage.

\subsection{Indentation Parsing} \label{design-indent}

The indentation parser takes the token stream generated by the Lexer, and
produces a different token stream which can be parsed in a context-free manner.
Most tokens are passed through this stage unmodified; the only tokens which are
changed or removed are the \texttt{COMMENT}, \texttt{NEWLINE} and
\texttt{SPACES} tokens.

Indentation parsing is achieved as follows: the token stream is split into
groups of tokens, breaking at each \texttt{NEWLINE} token; the \texttt{COMMENT}
tokens are removed, and \texttt{SPACES} tokens in each line are removed, unless
they are indentation\footnote{i.e. they are the first token on a line, and the
next token is not whitespace.}; finally, the remaining \texttt{SPACES} tokens
are all indentation, and can be converted into \texttt{INDENT} and
\texttt{DEDENT} tokens whenever the level of indentation is increased or
decreased between lines. By performing this transformation, the task of parsing
the new token stream is now context-free.

\subsection{Syntax Parsing} \label{design-parser}

Taking the output of the indentation parser as input, it is now possible to
parse the grammar with a collection of \gls{cfg} rules. The full grammar for
Occam is quite long and complex, so the majority of these rules were sourced in
part from \cite[p.~124]{jones}, and otherwise from
\cite[p.~106]{occ21}\footnote{This manual is actually for a later version of
Occam, which means that although many of the grammar rules do apply to the first
version of Occam, it contains several that do not.}.

One possibility for parsing would be to use an existing utility such as
\textit{Happy}\footnote{\url{https://www.haskell.org/happy/}} to generate a
parser from the grammar. However, such utilities often require a great deal of
effort in order to have them correctly track line numbers and columns for the
sake of generating understandable syntax errors, and are relatively easy to do
without in a language such as Haskell. Thus, I have elected to implement the
parser from scratch.

The parser consists of two main parts. The first part is a collection of
functions which describe parsing behaviour for each of a collection of
context-free combinators, as well as reduction rules which are used to construct
the \textit{\gls{ast}} from the source. This loosely acts as an alternative to a
parser generator, but makes it much easier to track source locations for error
messages. The second part is an expression of each of the grammar rules of Occam
in terms of these combinators. Together, they form a complete parser for the
Occam language.

\subsection{Semantic Checking}

Successful completion of the parsing phase implies that the source code is
grammatically sound. However, it is still possible that the source code does not
make sense. Consider the following example:
\lstinputlisting[language=occam]{code/semantic_error.occ}
Grammatically, there is nothing wrong with the line \texttt{c := 2}. However,
semantically, it makes no sense to assign an integer value to a channel. Thus,
it is necessary to perform semantic analysis to ensure that the program makes
sense. The list of checks that the semantic analyser could feasibly perform is
practically endless, but many are beyond the scope of this project. Checks that
the analyser will perform include:
\begin{itemize}
  \item Checks that variables are defined before they are used,
  \item Checks that types match the requirements in expressions and statements,
  \item Verification that constant-expressions are compile-time computable,
\end{itemize}
among other simple assertions. The analyser may also warn about things which are
not categorically errors, but resemble common programmer mistakes, or could have
unexpected behaviour. Alongside these checks, the analyser is responsible for
performing other transforms to the \gls{ast} in order to make it easier to
generate code from the resulting tree.

\subsection{Code Generation} \label{code-gen}

Conceptually the simplest step, but practically one of the most challenging,
is code generation. Taking the \gls{ast} as input, each program construct can be
compiled into a sequence of instructions that implement the desired behaviour.
Since program constructs can nest, code generation is a highly recursive
process, and is one of the main reasons behind choosing a functional language to
perform the compilation.

The code generation is factorised into a collection of functions, with at least
one per program construct. Although code generation is mostly a
functionally-pure process, it is necessary to be able to generate unique labels
throughout, so the functions are implemented monadically.

\subsection{Static Data} \label{static-blob}

Static data such as string constants or tables must somehow be transferred from
the source code to the virtual machine. One option would be to have a textual
representation for data in the assembly language syntax, but this would
unnecessarily complicate the assembler. Instead, the compiler will produce a
second file which contains this binary data in a form which is ready to be
passed to the virtual machine. The exact structure of this data file is
described in \ref{binary-format}.

\subsection{Peephole Optimization}

The code produced by the compiler may be suboptimal due to the recursive
construction: it may be the case that small but simple local optimisations could
improve the performance of the program by removing unnecessary instructions.
This could feasibly be performed by a so-called peephole optimisation phase at
the end of the compilation which would scan a sliding window over the assembly
code, looking for simple substitutions that could be performed. However, this
stage is not necessary for the compiler to function, and is much lower priority
than the majority of the other features of the project.

\section{Assembler}

Transputer Assembler is a peculiar combination consisting mostly of
\gls{risc}-style instructions, but including a small number of oddly high-level
operations. In reality, the instruction set of the transputer was implemented in
microcode for a much simpler underlying processor, but a virtual machine can
implement these instructions directly.

\subsection{Assembler Syntax} \label{asm-syntax}

In order to make parsing simple, the syntax for the assembler is very basic.
Each line in the input consists (in the following order) of:
\begin{itemize}
  \item An optional label, e.g. \texttt{START:}
  \item An optional \gls{mnemonic}, e.g. \texttt{opr}
  \item If an instruction was given, an optional argument. This can either be
        numeric (e.g. \texttt{-123}, \texttt{0xDEADBEEF}) or symbolic
        (e.g. \texttt{FOO} or \texttt{BAR - BAZ}).
  \item An optional comment, e.g. \texttt{\# Hello, World!}
\end{itemize}
This syntax makes the assembly language easy to read and understand, and the
inclusion of comments both allows human authors to describe what they are doing,
and for the compiler to justify its output for the purpose of debugging.

\subsection{Symbolic Labels}

The syntax description in the previous section mentions symbolic labels. The
purpose of these labels is to make it easier to mark locations in the source
code for the purpose of jumping to instructions. The most obvious way to assign
values to these labels would be to assign them values based upon where the
instructions are actually located in memory.

However, many of the instructions concerning code locations require the
\textit{relative} location of the instructions, rather than the
\textit{absolute} location. For example, the jump instruction \texttt{j}
requires a relative offset, and computing this relative value manually would be
inconvenient. Therefore, the assembler will have to somehow address this issue
in order to allow instructions like \texttt{j} to take only a label as an
argument, but for other instructions such as \texttt{startp} to be able to
compute offsets between labels.

The solution to this is delightfully straightforward: rather than having a label
expand to the absolute location of its definition, the label can be expanded to
the \textit{offset} from the reference
location\textsuperscript{\cite{supervisor}}. This allows \texttt{j} to use the
label as the argument, whilst leaving label difference calculations unaffected.
Absolute locations can be computed with the combination of \texttt{ldc LABEL}
and \texttt{ldpi}\footnote{\texttt{ldpi} loads the value of the instruction
pointer and adds it to the value in register A.}.

\subsection{Variable-length Instructions} \label{var-len}

As explained in \ref{ops}, the \Gls{transputer} has only 13 \gls{direct}s. The
rest of the instruction set is accessed via the \texttt{opr} instruction, and
are referred to as \gls{indirect}s. In practice, the most frequently executed
instructions have the shortest byte sequences. This means that the size of the
code is much smaller than it would be if the instructions were encoded using a
fixed-width encoding\footnote{In truth, the instruction set of the Transputer
was not variable-length for this reason. Instead, the variable-length encoding
allowed programs to be compiled in such a way that they were fully transferrable
between machines with differing native word lengths. For this project, however,
the main advantage is reducing the size of the code.}. However, the assembler
can perform this indirection automatically, allowing the programmer to write
\texttt{xor} and have the assembler generate the sequence \texttt{pfix 3, opr 3}
which executes the same thing. Additionally, direct instructions may take longer
arguments by augmenting them using the \texttt{pfix} and \texttt{nfix}
instructions.

Having this automatic conversion is convenient for the programmer, but also
introduces a few difficulties in the assembler. Symbolic labels have to be
expanded before the instructions referencing them can be variable-length
encoded. Awkwardly, the value of labels might be affected by the length of
the instructions that reference them. The length of the encoded instruction
might even affect what value the instruction should be encoding.
Consider the following example:
\lstinputlisting[language=assembly]{code/variable_length.s}

The length of the instruction \texttt{j FOO} depends upon how far back
\texttt{FOO} is. However, the label \texttt{FOO} has to be expanded relative to
the \textit{last} byte of the jump, so it is necessary to encode the
instruction, in order to compute the offset for \texttt{FOO}, which is needed so
that the instruction can be encoded in the first place.

To address this, the assembler will need to repeatedly recompute the label
values and the instruction lengths until a fixed point is reached. The value of
the operand only increases or decreases linearly with the change in the value of
the label, and the length of the instruction only increases with the log of the
offset. This means that given reasonable initial values for the labels, a
bounded number of iterations is sufficient to find a fixed point\footnote{I
haven't proven that this is the case, but all programs that I have tried to
assemble so far have assembled with no more than 5-10 iterations.}.

\section{Virtual Machine}

The virtual machine will execute the bytecode produced by the assembler. It will
implement most of the Transputer instruction set, with some exceptions and
alterations. Some of the more complicated Transputer instructions are wholly
unnecessary for implementing the version of Occam which I am aiming to
implement\footnote{Two-dimensional block move instructions and floating point
support, to name a few.}. Additionally, it is convenient to have direct
instructions for printing values to the terminal, so that programs can display
debugging information.

\subsection{Target Executable Format} \label{binary-format}

There are two parts to the programs that the compiler will produce:
instructions and data. The instructions will be output as Transputer assembler
which is then assembled into bytecode, but the data is provided separately as
the binary blob mentioned in \ref{static-blob}. The data file will consist of a
collection of records, each containing:
\begin{itemize}
  \item
    \texttt{int32le address} - The address of the start of this data in the VM
    memory.
  \item
    \texttt{int32le length} - The length (in bytes) of the data block in this
    record.
  \item
    \texttt{byte data[length]} - The data to be loaded into the VM memory.
\end{itemize}

\section{Runtime System}

The runtime system described in \ref{dist-system} can be broken into three main
components: Instances (which run the code), the Process Server (which maintains
a collection of instances), and the Process Master (which maintains a collection
 of process servers).

\subsection{Instances}

An instance is a specialisation of the virtual machine which provides additional
functionality to allow the program to communicate with other instances. This
functionality includes:
\begin{itemize}
  \item The ability to spawn new instances, potentially on a different machine.
  \item The ability to communicate via channels with other instances, provided
        that the two instances both share access to the same channel.
\end{itemize}

When a process requests a new instance, it provides the address of the
instruction which that new instance should execute next, as well as specifying
how much memory will be required by the new instance. It also implicitly
provides its own workspace pointer, which is used as the initial workspace
address of the new instance. The reason for doing this is so that the newly
created instance is capable of addressing channels which were allocated in the
workspace of the parent instance. The channel becomes uniquely identifiable by
its address, and the ID of the instance that allocated it.

\subsection{Process Server}

The process server is responsible for hosting several instances of the virtual
machine. The server has one copy of the bytecode and one copy of the static data
for the entire program, which it shares with the VM instances that rely on them.
The process server is started by the process master on one of the worker
machines, and is stopped when the program terminates.

The main responsibility of the process server is to act as an intermediary for
communications between VM instances and the process master, but it can also
perform some communications internally. If two separate instances hosted by the
same process server are trying to communicate, it would be inefficient to
forward the communications to the process master only to have it returned.
Instead, the message can be directly copied from the memory of one instance into
the memory of the other.

\subsection{Process Master} \label{process-master}

The process master is what initiates the execution of a program. The user
provides a \textit{job file}, which is a JSON file containing configuration for
the job. This specifies the code and data files, as well as listing the
host/port addresses of each of the worker machines that should be used for the
task. The process master connects to each of these and instructs them each to
spawn a new process server. It then initiates the root process on one of the
process servers and waits for that process to finish before exiting. Whilst it
is waiting, it serves the following tasks:

\begin{itemize}
  \item
    Instances running on any of the process servers may request the process
    master to spawn new instances to run sub-processes. The master decides where
    to create these instances and then instructs the corresponding process
    server to start running that instance.
  \item
    An instance may attempt to communicate with any channel that it can address.
    If this results in two instances communicating with each other from separate
    process servers, then the communication has to be resolve via the process
    master.
\end{itemize}
